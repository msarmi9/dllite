{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neural-Nets-Are-Not-Black-Boxes\" data-toc-modified-id=\"Neural-Nets-Are-Not-Black-Boxes-1\">Neural Nets Are Not Black Boxes</a></span></li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-2\">Objective</a></span></li><li><span><a href=\"#Back-propagation\" data-toc-modified-id=\"Back-propagation-3\">Back-propagation</a></span></li><li><span><a href=\"#Binary-Cross-Entropy\" data-toc-modified-id=\"Binary-Cross-Entropy-4\">Binary Cross Entropy</a></span></li><li><span><a href=\"#Activations\" data-toc-modified-id=\"Activations-5\">Activations</a></span></li><li><span><a href=\"#Linear-Layer\" data-toc-modified-id=\"Linear-Layer-6\">Linear Layer</a></span></li><li><span><a href=\"#Putting-It-All-Together\" data-toc-modified-id=\"Putting-It-All-Together-7\">Putting It All Together</a></span></li><li><span><a href=\"#Our-Evaluation-Metric\" data-toc-modified-id=\"Our-Evaluation-Metric-8\">Our Evaluation Metric</a></span></li><li><span><a href=\"#Trainer\" data-toc-modified-id=\"Trainer-9\">Trainer</a></span></li><li><span><a href=\"#Pre-process-Data\" data-toc-modified-id=\"Pre-process-Data-10\">Pre-process Data</a></span></li><li><span><a href=\"#Datasets-&amp;-DataLoaders\" data-toc-modified-id=\"Datasets-&amp;-DataLoaders-11\">Datasets &amp; DataLoaders</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-12\">Train</a></span></li><li><span><a href=\"#Comparison-with-sklearn\" data-toc-modified-id=\"Comparison-with-sklearn-13\">Comparison with sklearn</a></span></li><li><span><a href=\"#Adding-More-Layers\" data-toc-modified-id=\"Adding-More-Layers-14\">Adding More Layers</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-15\">Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets Are Not Black Boxes \n",
    "\n",
    "If you think neural nets are black boxes, you're certainly not alone. While they may not be as interpretable as models like random forests (at least not yet), we can still understand how networks process data to arrive at their predictions, and that's exactly what we'll do in this post. We'll build our own network from scratch, starting with logistic regression.\n",
    "\n",
    "This post is very much inspired by [this fantastic post](https://sgugger.github.io/a-simple-neural-net-in-numpy.html#a-simple-neural-net-in-numpy) by Sylvain Gugger. We won't pretend to improve upon Sylvain's post; we just want to explain things in our own way to help us understand things a little bit more clearly. This will be the first of a series of posts in which we'll write our own DNN, CNN, and RNN. You can find the source code for all of these posts at [tinytorch](https://github.com/msarmi9/tinytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.814191Z",
     "start_time": "2021-03-05T03:37:42.189036Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.818831Z",
     "start_time": "2021-03-05T03:37:42.816428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 9\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective \n",
    "\n",
    "Our goal is to construct a binary logistic classifier as a neural network. The network will consist of a single linear layer followed by a sigmoid activation with binary cross entropy as the loss. We'll begin by deriving the back-prop equations for our particular scenario and in doing so we'll see that what we've done generalizes immediately to networks with arbitrary layers and activations. In other words, we'll have developed a framework that can model any feedforward network--all by starting from ordinary logistic regression. \n",
    "\n",
    "Actually, this isn't all that surprising when you think about it. Logistic regression is a linear layer followed by sigmoid and feedforward networks are just a bunch of linear layers stacked together with non-linear activations in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-propagation\n",
    "\n",
    "Back-propagation is nothing more than the chain rule. We can view our logistic network as the composition of three functions\n",
    "\n",
    "$$x \\to \\text{BCE} \\circ \\text{Sigmoid} \\circ \\text{Linear}(x)$$\n",
    "\n",
    "While the loss function is not usually viewed as a layer of the network, treating it as the final layer makes computing the gradients easier. Let's denote the output of the $i$-th layer by $x_i$ so that\n",
    "\n",
    "\\begin{align}\n",
    "    x_1 &= \\text{Linear}(x)     \\\\\n",
    "    x_2 &= \\text{Sigmoid}(x_1)  \\\\\n",
    "    x_3 &= \\text{BCE}(x_2)\n",
    "\\end{align}\n",
    "\n",
    "The first gradient we have to compute is the gradient of $\\text{BCE}$ with respect to the activations $x_2$.\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_2} = \\frac{\\partial \\text{BCE}}{\\partial x_2}(x_2) $$\n",
    "\n",
    "Next we have to compute the gradient with respect to the linear outputs $x_1$. The chain rule tells us\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_1} = \\frac{\\partial \\text{BCE}}{\\partial x_2} \\times \\frac{\\partial \\text{Sigmoid}}{\\partial x_1}(x_1)$$\n",
    "\n",
    "Last, we'll need to compute the gradient with respect to the original inputs $x$\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x} = \\frac{\\partial \\text{BCE}}{\\partial x_1} \\times \\frac{\\partial \\text{Linear}}{\\partial x_1}(x)$$\n",
    "\n",
    "Notice a pattern? The first gradient we computed--the gradient with respect to the network's final activations--is used to compute the next gradient--the gradient with respect to the linear outputs--which are in turn used to compute the gradient with respect to the original inputs. To compute the gradients of any network, we simply start at the last layer and successively pass the gradients backwards to the preceding layer until we arrive at the original inputs. That's the reason it's called back-propagation. It really is helpful to picture passing the gradients backwards through the network like a baton.\n",
    "\n",
    "We'll compute each of these gradients in turn, starting with the last layer and working our way backwards to the original inputs.\n",
    "\n",
    "__Note:__ So far we've been treating the input $x$ as a single variable, but most of the time $x$ will have more than one dimension. Computing the gradients in the multi-variate case isn't anymore difficult than what we've done (it involves something called the Jacobian, but we'll pretend we didn't hear that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Cross Entropy\n",
    "\n",
    "Binary cross entropy penalizes predictions by the logarithm of their confidence. Given labels $y$ which are either zero or one and probabilities $\\hat{y}$ for the positive class, we add $-\\ln(\\text{P}(y=1))$ to the loss whenever $y = 1$ and $-\\ln(\\text{P}(y=0))$ whenever $y = 0$. In other words,\n",
    "\n",
    "$$\\text{BCE}(\\hat{y}, y) = -[y \\ln(\\hat{y}) + (1 - y)\\ln(1 - \\hat{y})]$$\n",
    "\n",
    "After simplifying, you'll find its derivative is\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial \\hat{y}} = \\frac{\\hat{y} - y}{\\hat{y}(1 - \\hat{y})}$$\n",
    "\n",
    "To avoid potential division-by-zero errors, we'll clip the probabilities $\\hat{y}$ so that they're not too close to zero or one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.825709Z",
     "start_time": "2021-03-05T03:37:42.821702Z"
    }
   },
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy:\n",
    "    \"\"\"Container for the forward and backward pass of BCE.\"\"\"\n",
    "    \n",
    "    def forward(self, y_hat, y):\n",
    "        \"\"\"Return binary cross entropy given predictions and targets.\"\"\"\n",
    "        self.y_hat, self.y = y_hat.clip(min=1e-8, max=1-1e-8), y\n",
    "        return -np.where(y==1, np.log(self.y_hat), np.log(1 - self.y_hat))\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate the gradient with respect to predictions.\"\"\"\n",
    "        return (self.y_hat - self.y) / (self.y_hat * (1 - self.y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "\n",
    "The easiest components of networks to handle are the activation functions. Our activation is sigmoid, which you'll often see defined as one of \n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)} \\quad \\text{or} \\quad \\frac{\\text{exp}(x)}{1 + \\text{exp}(x)}$$\n",
    "\n",
    "It turns out we need both versions to implement a numerically stable sigmoid. Why? Notice how when $x$ is very negative, $\\text{exp}(-x)$ is very large, and when $x$ is very positive, $\\text{exp}(x)$ is very large--in both cases too large to store in memory. The easy fix is to use the former when $x > 0$ and the latter when $x < 0$.\n",
    "\n",
    "After simplifying, you'll find the derivative of sigmoid is\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "Notice something interesting? Since we denoted $\\hat{y}$ above as the output of sigmoid, this is exactly the denominator of the BCE gradient we just computed, meaning the two terms will cancel when we compute the gradient with respect to the outputs $x_1$ of our network's last (and only) linear layer. After canceling, we're left with\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial x_1} = \\hat{y} - y $$\n",
    "\n",
    "Nice, right? This tells us that the gradient of the loss with respect to the network's final linear outputs is just the difference between the probabilities $\\hat{y}$ and the labels $y$. The further apart they are (i.e. the worse our predictions are), the larger the gradient and the larger the update to the last linear layer's weights in the SGD step (remember, the chain rule tells us the above gradient appears as a factor in the gradient with respect to the weights of the last linear layer).\n",
    "\n",
    "This is terrific because it means the weights of our network will change gradually as we train and won't spike or drop suddenly, which would be the case if the gradients were a quadratic or higher-order function of the prediction error. It also demonstrates nicely how a network adjusts its weights based on the error of its predictions.\n",
    "\n",
    "In fact, the same is true of multi-classification, where instead of sigmoid we use softmax (or log softmax) as the final activation and cross entropy as the loss. In this case, the gradient with respect to the last linear layer's weights $x$ is\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\text{CE}}{\\partial x}(\\hat{y}, y) = \\frac{\\partial \\text{CE}}{\\partial \\hat{y}}(\\hat{y}) \\times \\frac{\\partial \\text{softmax}}{\\partial x}(x) = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "We might cover the multi-class case in more depth in a follow-up post, although it's more or less the same as what we've done so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.832840Z",
     "start_time": "2021-03-05T03:37:42.828848Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"Container for the forward and backward pass of sigmoid.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a sigmoid layer.\"\"\"\n",
    "        self.y_hat = np.where(x > 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n",
    "        return self.y_hat\n",
    "        \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the preceding gradient.\"\"\"\n",
    "        return self.y_hat * (1 - self.y_hat) * grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer\n",
    "\n",
    "The last and most difficult component we need to implement is the linear layer, which contains weights and biases. Denoting the linear outputs by $z$, we have\n",
    "\n",
    "$$z = xw + b$$\n",
    "\n",
    "If $x$ is a mini-batch of shape $(bs, n_{inp})$, then $w$ has shape $(n_{inp}, 1)$ and $b$ has shape $(1,)$, with addition being done via broadcasting. To make things easier, for the moment let's just imagine we have a batch size of one.\n",
    "\n",
    "$$x = [x_1, \\dots, x_{n_{inp}}]$$ \n",
    "\n",
    "There are two gradients to compute this time around, one with respect to the weights and another with respect to the bias. To make life easier still, let's write everything out in coordinates.\n",
    "\n",
    "$$z_i = \\sum_{k=1}^{n_{inp}} x_k w_{ki} + b$$\n",
    "\n",
    "Taking the derivative with respect to the weights, we get\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial w_{ki}} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}\n",
    "{\\partial w_{ki}} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times x_k$$\n",
    "\n",
    "Taking a closer look at these gradients, we see exactly why neural nets are so sensitive to the scale of their inputs. Because the gradients with respect to the weights $w$ are scaled by the input features $x$, having features of different magnitudes will result in some gradients being larger than others. These larger gradients will dominate the learning process and prevent the network from learning from all features equally. This is why it's important to always normalize your data before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative with respect to the bias, we have\n",
    "\n",
    "$$\\frac{\\partial \\text{BCE}}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial b} = \\frac{\\partial \\text{BCE}}{\\partial z_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice something nice? Since we'll already have the gradient with respect to the linear outputs $z$ stored beforehand in a variable called $\\text{grad}$, we get the gradient with respect to the bias for free, leaving just the weights to deal with. The main obstacle is figuring out how to write the above equations as a matrix product. Whenever I have to do something like this, I just focus on getting the shapes right.\n",
    "\n",
    "\\begin{align}\n",
    "  &\\bullet x \\text{ has shape } (bs, n_{inp}) \\\\\n",
    "  &\\bullet \\text{grad has shape } (bs, 1) \\\\\n",
    "  &\\bullet \\text{grad}_W \\text{ has shape } (n_{inp}, 1)\n",
    "\\end{align}\n",
    "\n",
    "The only way we can multiply $x$ and $\\text{grad}$ and get something of shape $(n_{inp}, 1)$ is to re-shape $x$ to have shape $(bs, n_{inp}, 1)$ and $\\text{grad}$ to have shape $(bs, 1, 1)$ so that ordinary matrix multiplication over the last two dimensions gives the shape $(n_{inp}, 1)$.\n",
    "\n",
    "Were there another linear layer we'd also need to compute the gradient with respect to the inputs $x$ so we could keep back-propagating the gradients. This isn't anymore complicated than what we've done so far and doing so will allow us to build a network with any number of layers, so let's go ahead and do it. Since $x_k$ appears in each of the activations $z_i$, the gradient will respect to $x_k$ will involve summing all of the intermediate gradients with respect to $z_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial \\text{BCE}}{x_k} = \\sum_{i=1}^{n_{inp}} \\frac{\\partial \\text{BCE}}{\\partial z_i} \\times \\frac{\\partial z_i}{\\partial x_k} = \\sum_{i=1}^{n_{inp}} \\frac{\\partial \\text{BCE}}{\\partial z_i} w_{ki}$$\n",
    "\n",
    "We can re-write this as a matrix product using the transpose of the weight matrix.\n",
    "\n",
    "$$ \\frac{\\partial \\text{BCE}}{\\partial x_k} = \\text{grad} \\times W^t$$\n",
    "\n",
    "Let's do a sanity check on the dimensions involved to make sure nothing has gone horribly wrong. Since $\\text{grad}$ has shape $(bs, 1)$ and $W$ has shape $(n_{inp}, 1)$, $\\text{grad} \\times W^t$ has shape $(bs, n_{inp})$, which is exactly the shape of $x$--just as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.838934Z",
     "start_time": "2021-03-05T03:37:42.834385Z"
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Container for the forward and backward pass of a linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inp, n_out):\n",
    "        \"\"\"Initialise layer with random weights and zero bias.\"\"\"\n",
    "        k = 1 / np.sqrt(n_inp)\n",
    "        self.weights = np.random.uniform(-k, k, (n_inp, n_out))\n",
    "        self.bias = np.zeros(n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through a linear layer.\"\"\"\n",
    "        self.x = x\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Backpropagate the gradient given the preceding gradient.\"\"\"\n",
    "        self.grad_w = (self.x[:,:,None] @ grad[:,None,:]).mean(axis=0)\n",
    "        self.grad_b = grad.mean(axis=0)\n",
    "        return grad @ self.weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "It's finally time to string together all of the work we've done so far into a complete network. Then we'll put it to the test on the [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) and see how we compare to sklearn's built-in logistic model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.844000Z",
     "start_time": "2021-03-05T03:37:42.840404Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    \"\"\"Container for a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers, criterion):\n",
    "        \"\"\"Initialise layers and loss criterion.\"\"\"\n",
    "        self.layers = layers\n",
    "        self.criterion = criterion\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through the network.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        \"\"\"Backpropagate gradients to the start of the network.\"\"\"\n",
    "        grad = self.criterion.backward()\n",
    "        for layer in self.layers[::-1]:\n",
    "            grad = layer.backward(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.848729Z",
     "start_time": "2021-03-05T03:37:42.845550Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"Container for updating a model's weights via SGD.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr):\n",
    "        \"\"\"Initialise model parameters and learning rate.\"\"\"\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "                  \n",
    "    def step(self):\n",
    "        \"\"\"Update weights and biases of all linear layers.\"\"\"\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.weights -= self.lr * layer.grad_w\n",
    "                layer.bias -= self.lr * layer.grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Evaluation Metric\n",
    "\n",
    "For simplicity, we'll just consider accuracy as our evaluation metric for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.854495Z",
     "start_time": "2021-03-05T03:37:42.851970Z"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    \"\"\"Compute accuracy given soft binary predictions.\"\"\"\n",
    "    y_pred = y_hat > 0.5\n",
    "    return (y_pred == y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer\n",
    "\n",
    "To make life easier, let's wrap all of the functionality we'll need to train a network in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.863990Z",
     "start_time": "2021-03-05T03:37:42.856844Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"Container for training a feedforward neural net.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, optimizer, train_dl, val_dl, metric):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.val_dl = val_dl\n",
    "        self.metric = metric\n",
    "        \n",
    "    def train_one_epoch(self):\n",
    "        \"\"\"Train for one epoch and return the loss.\"\"\"\n",
    "        loss, n = 0, 0\n",
    "        for x, y in self.train_dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y_hat, y).sum()\n",
    "            self.model.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n\n",
    "            \n",
    "    def train(self, n_epochs, log_level=1):\n",
    "        \"\"\"Train for several epochs.\"\"\"\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = self.train_one_epoch()\n",
    "            val_loss, val_metric = self.evaluate(self.val_dl)\n",
    "            if (epoch + 1) % log_level == 0:\n",
    "                print(f\"epoch= {epoch:2d} | loss= {loss:.3f} | \"\n",
    "                      f\"val_loss= {val_loss:.3f} | val_metric= {val_metric:.3f}\")\n",
    "    \n",
    "    def evaluate(self, dl):\n",
    "        \"\"\"Return loss and metric on validation or test set.\"\"\"\n",
    "        loss, n, metric = 0, 0, 0\n",
    "        for x, y in dl:\n",
    "            y_hat = self.model.forward(x)\n",
    "            batch_loss = self.model.criterion.forward(y_hat, y).sum()\n",
    "            batch_metric = self.metric(y_hat, y)\n",
    "            metric += len(y) * batch_metric\n",
    "            loss += batch_loss\n",
    "            n += len(y)\n",
    "        return loss / n, metric / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process Data\n",
    "\n",
    "We'll use sklearn's [breast cancer dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer) for our binary classification task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.881638Z",
     "start_time": "2021-03-05T03:37:42.866015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Train-test-split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.892216Z",
     "start_time": "2021-03-05T03:37:42.888978Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize(X_train, X_val):\n",
    "    \"\"\"Normalize training and validation data using training stats.\"\"\"\n",
    "    for j in range(X_train.shape[1]):\n",
    "        mu, sigma = X_train[:,j].mean(), X_train[:,j].std()\n",
    "        X_train[:,j] = (X_train[:,j] - mu) / sigma\n",
    "        X_val[:,j] = (X_val[:,j] - mu) / sigma\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.897554Z",
     "start_time": "2021-03-05T03:37:42.893660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize with training stats\n",
    "X_train, X_val = normalize(X_train, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets & DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train in batches, we'll need to implement our own version of pytorch's datasets and dataloaders, since we're doing everything in numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.903037Z",
     "start_time": "2021-03-05T03:37:42.899068Z"
    }
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"Container for returning inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"Initialise inputs and re-shape targets as a column vector.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    def __setitem__(self, idx, val):\n",
    "        self.X[idx], self.y[idx] = val\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.909718Z",
     "start_time": "2021-03-05T03:37:42.904559Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Container for returning a mini-batch of inputs and targets.\"\"\"\n",
    "    \n",
    "    def __init__(self, ds, batch_size, shuffle=False):\n",
    "        \"\"\"Initialise dataset and batch size.\"\"\"\n",
    "        self.ds = ds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def shuffle_data(self):\n",
    "        \"\"\"Shuffle inputs and targets.\"\"\"\n",
    "        idxs = np.random.permutation(len(self.ds))\n",
    "        self.ds = Dataset(*self.ds[idxs])\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a mini-batch of inputs and targets.\"\"\"\n",
    "        if self.shuffle: self.shuffle_data()\n",
    "        n_batches = len(self.ds) // self.batch_size\n",
    "        for i in range(n_batches):\n",
    "            yield self.ds[i * self.batch_size: (i + 1) * self.batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.914108Z",
     "start_time": "2021-03-05T03:37:42.911217Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_ds = Dataset(X_train, y_train)\n",
    "val_ds = Dataset(X_val, y_val)\n",
    "\n",
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=len(X_val), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Now we're ready to put our model to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.932765Z",
     "start_time": "2021-03-05T03:37:42.915634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.421 | val_loss= 0.263 | val_metric= 0.939\n",
      "epoch=  1 | loss= 0.246 | val_loss= 0.198 | val_metric= 0.956\n",
      "epoch=  2 | loss= 0.199 | val_loss= 0.169 | val_metric= 0.965\n",
      "epoch=  3 | loss= 0.170 | val_loss= 0.152 | val_metric= 0.965\n",
      "epoch=  4 | loss= 0.158 | val_loss= 0.140 | val_metric= 0.965\n",
      "epoch=  5 | loss= 0.144 | val_loss= 0.132 | val_metric= 0.965\n",
      "epoch=  6 | loss= 0.138 | val_loss= 0.125 | val_metric= 0.965\n",
      "epoch=  7 | loss= 0.131 | val_loss= 0.120 | val_metric= 0.965\n",
      "epoch=  8 | loss= 0.126 | val_loss= 0.116 | val_metric= 0.965\n",
      "epoch=  9 | loss= 0.120 | val_loss= 0.112 | val_metric= 0.965\n"
     ]
    }
   ],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.1)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl, metric)\n",
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with sklearn\n",
    "\n",
    "Let's see how our logistic network stacks up against sklearn's logistic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.946850Z",
     "start_time": "2021-03-05T03:37:42.934242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're close!\n",
    "sklearn_model = LogisticRegression(random_state=seed)\n",
    "sklearn_model.fit(X_train, y_train)\n",
    "sklearn_model.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding More Layers\n",
    "\n",
    "Now let's see if we can do a bit better by training a deeper network. First, we'll need to implement $\\text{ReLU}$ for the activations in between our linear layers. $\\text{ReLU}$ functions as a gate that only admits positive inputs and sends all negative inputs to zero.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{ReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You might recognize its derivative as the Heaviside function, which models an electric current that's turned on at time $t = 0$.\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ReLU}'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "During the backward pass $\\text{ReLU}$ again functions as a gatekeeper that only admits gradients corresponding to positive inputs and sends all other gradients to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.952088Z",
     "start_time": "2021-03-05T03:37:42.948885Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"Container for the forward and backward pass of ReLU.\"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Pass a mini-batch through ReLU.\"\"\"\n",
    "        self.x = x\n",
    "        return np.where(x > 0, x, 0)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"Return the gradient where x is positive, otherwise zero.\"\"\"\n",
    "        return np.where(self.x > 0, grad, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-05T03:37:42.990706Z",
     "start_time": "2021-03-05T03:37:42.953937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=  0 | loss= 0.617 | val_loss= 0.496 | val_metric= 0.930\n",
      "epoch=  1 | loss= 0.457 | val_loss= 0.352 | val_metric= 0.939\n",
      "epoch=  2 | loss= 0.340 | val_loss= 0.258 | val_metric= 0.939\n",
      "epoch=  3 | loss= 0.258 | val_loss= 0.204 | val_metric= 0.939\n",
      "epoch=  4 | loss= 0.210 | val_loss= 0.173 | val_metric= 0.939\n",
      "epoch=  5 | loss= 0.183 | val_loss= 0.153 | val_metric= 0.939\n",
      "epoch=  6 | loss= 0.162 | val_loss= 0.139 | val_metric= 0.939\n",
      "epoch=  7 | loss= 0.141 | val_loss= 0.128 | val_metric= 0.947\n",
      "epoch=  8 | loss= 0.133 | val_loss= 0.120 | val_metric= 0.965\n",
      "epoch=  9 | loss= 0.122 | val_loss= 0.114 | val_metric= 0.974\n"
     ]
    }
   ],
   "source": [
    "# Input and final output dims\n",
    "n_inp = X_train.shape[1]\n",
    "\n",
    "# Initialise layers and criterion\n",
    "metric = accuracy\n",
    "criterion = BinaryCrossEntropy()\n",
    "layers = [Linear(n_inp, 20), ReLU(), Linear(20, 1), Sigmoid()]\n",
    "model = Sequential(layers, criterion)\n",
    "\n",
    "# Initialise optimizer and trainer\n",
    "optimizer = SGD(model, lr=0.10)\n",
    "trainer = Trainer(model, optimizer, train_dl, val_dl, metric)\n",
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all for today. Thanks for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
